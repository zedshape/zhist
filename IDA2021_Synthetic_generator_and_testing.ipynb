{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from utils import *\n",
    "from ZHist import *\n",
    "from ZMiner import *\n",
    "from ZMinerD import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make two different pairs of two sets:\n",
    "1. Create a normal set (1000) and an abnormal set (500)\n",
    "2. Create a histogram variable with fixed number of bins and fixed number of time duration\n",
    " - fixed number of bins (10, 12, 14) \n",
    "     -> as we convert it to the distance from the central point, it does not affect much\n",
    " - fixed number of time duration (3 hours, 6 hours, 9 hours) for each snapshot\n",
    "     -> it will work as a weight given to each snapshot of a specific bin when getting a central point\n",
    " - the number of snapshot -> the total time will be different time durations * number of snapshot\n",
    "  for each bin $b$, $\\sum_i^{s_b}{t_b(i)}$\n",
    "  \n",
    "3. logic\n",
    " - for a normal set: just follow a normal distribution\n",
    "   - mean and std are randomly selected\n",
    " - for an abnormal set:\n",
    "   - start from the same distribution with \n",
    "   - Markov simulation\n",
    "     - for mean: multiply previous mean by 0.01/0.05/0.1 with the probability 0.5\n",
    "     - for std: multiply previous std by 0.01/0.05/0.1 with the probability 0.5\n",
    "   - we can change those two values to check the effect of both parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createSyntheticDataset(n_variables, bin_nos, durations, n_snapshots, n_normal, \n",
    "                          n_abnormal, prob_mean, prob_std, weight_mean, weight_std):\n",
    "    # duration for each (car, snapshot)\n",
    "    # need to be pre-defined to keep consistency\n",
    "    selected_durations = {\n",
    "        \"normal\": {},\n",
    "        \"abnormal\": {},\n",
    "    }\n",
    "    normal_dfs = []\n",
    "    abnormal_dfs = []\n",
    "    multicolumns = []\n",
    "    columns = []\n",
    "\n",
    "\n",
    "    for n_car in range(n_normal):\n",
    "        n_snap_selected = np.random.choice(n_snapshots, 1)[0]\n",
    "\n",
    "        if n_car not in selected_durations[\"normal\"]:\n",
    "            selected_durations[\"normal\"][n_car] = {}\n",
    "        for n_snap in range(n_snap_selected):\n",
    "            selected_durations[\"normal\"][n_car][n_snap] = np.random.choice(durations, 1)[0]\n",
    "\n",
    "    for n_car in range(n_abnormal):\n",
    "        n_snap_selected = np.random.choice(n_snapshots, 1)[0]\n",
    "\n",
    "        if n_car not in selected_durations[\"abnormal\"]:\n",
    "            selected_durations[\"abnormal\"][n_car] = {}\n",
    "        for n_snap in range(n_snap_selected):\n",
    "            selected_durations[\"abnormal\"][n_car][n_snap] = np.random.choice(durations, 1)[0]\n",
    "\n",
    "    print(\"Snapshot & duration creation complete\")\n",
    "\n",
    "    # for each histogram parameters\n",
    "    for n in range(n_variables):\n",
    "        # create a variable with bins\n",
    "        n_bin_selected = np.random.choice(bin_nos, 1)[0]\n",
    "        bin_names = {b: chr(65+n)+str(b) for b in range(n_bin_selected)}\n",
    "\n",
    "        ####\n",
    "        # create columns\n",
    "\n",
    "        for b in bin_names.values():\n",
    "            multicolumns.append((chr(65+n), b))\n",
    "            columns.append(b)\n",
    "\n",
    "        ####\n",
    "        print(\"For variable\", n, \", the bins are\", bin_names)\n",
    "        print(\"=================================\")\n",
    "        print(\"Starting sampling...\")\n",
    "        \n",
    "        \n",
    "        # each histogram variable should have their own distribution\n",
    "        # For simplicity, we start everything from the standard normal distribution\n",
    "        # Those parameters can be changed for different simulation trials.\n",
    "        h_val_mean = 0\n",
    "        h_val_std = 1\n",
    "\n",
    "        ####### choose number of snapshots it will have \n",
    "        initial_time = 0\n",
    "\n",
    "        samples = {\n",
    "            \"normal\": {},\n",
    "            \"abnormal\": {},\n",
    "        }\n",
    "\n",
    "        total_samples_for_bin = []\n",
    "        # for normal car\n",
    "        for n_car in range(n_normal):\n",
    "            # for each snapshot\n",
    "            samples[\"normal\"][n_car] = {}\n",
    "            for n_snap in range(len(selected_durations[\"normal\"][n_car])):\n",
    "                #### random sampling\n",
    "                # select a total duration (total sampling trials)\n",
    "\n",
    "                duration_selected = selected_durations[\"normal\"][n_car][n_snap]\n",
    "\n",
    "                # random sample * duration time\n",
    "                samples[\"normal\"][n_car][n_snap] = [np.random.normal(h_val_mean, h_val_std) for _ in range(duration_selected)]\n",
    "                total_samples_for_bin += samples[\"normal\"][n_car][n_snap]\n",
    "        # for abnormal car\n",
    "        for n_car in range(n_abnormal):\n",
    "            # for each snapshot\n",
    "            samples[\"abnormal\"][n_car] = {}\n",
    "            # for abnormal group\n",
    "            # every trial we need to correct the distribution\n",
    "            h_val_mean_temp = h_val_mean\n",
    "            h_val_std_temp = h_val_std\n",
    "\n",
    "            for n_snap in range(len(selected_durations[\"abnormal\"][n_car])):\n",
    "                #### random sampling\n",
    "                # select a total duration (total sampling trials)\n",
    "                duration_selected = selected_durations[\"abnormal\"][n_car][n_snap]\n",
    "\n",
    "                samples_tmp = []\n",
    "\n",
    "                for _ in range(duration_selected):\n",
    "                    sample = np.random.normal(h_val_mean_temp, h_val_std_temp)\n",
    "                    samples_tmp.append(sample)\n",
    "                    #correct the distribution\n",
    "                    # mean \n",
    "                    if np.random.choice([1, 0], p=[prob_mean, 1-prob_mean]):\n",
    "                        h_val_mean_temp *= weight_mean\n",
    "                    # std\n",
    "                    if np.random.choice([1, 0], p=[prob_std, 1-prob_std]):\n",
    "                        h_val_std_temp *= weight_std\n",
    "\n",
    "                samples[\"abnormal\"][n_car][n_snap] = samples_tmp\n",
    "                total_samples_for_bin += samples[\"abnormal\"][n_car][n_snap]\n",
    "\n",
    "        # after finishing sampling (for each histogram variable), we need to create bins according to their values\n",
    "\n",
    "        #concatenated = np.concatenate(list(samples[\"normal\"][0].values())+list(samples[\"abnormal\"][0].values()))\n",
    "        out, bins = pd.qcut(total_samples_for_bin, n_bin_selected, retbins=True)\n",
    "        del total_samples_for_bin #for memory stability\n",
    "\n",
    "        # apply those bins to each normal/car and abnormal car\n",
    "\n",
    "        for n_car in range(n_normal):\n",
    "            normal_dict = []\n",
    "            for n_snap in range(len(selected_durations[\"normal\"][n_car])):\n",
    "                bin_tmp = pd.cut(samples[\"normal\"][n_car][n_snap], bins, labels=False)\n",
    "                normal_dict.append(Counter(bin_tmp))\n",
    "            normal_df = pd.DataFrame(normal_dict)\n",
    "            normal_df.rename(columns=bin_names, inplace=True)\n",
    "            if n == n_variables - 1:\n",
    "                normal_df[\"no\"] = n_car\n",
    "                normal_df[\"time\"] = selected_durations[\"normal\"][n_car].values()\n",
    "                normal_df[\"snap\"] = range(len(selected_durations[\"normal\"][n_car]))\n",
    "                normal_df[\"date\"] = normal_df[\"time\"].cumsum()\n",
    "            if n_car == 0:\n",
    "                normal_df_concat = normal_df\n",
    "            else:\n",
    "                normal_df_concat = pd.concat((normal_df_concat, normal_df))\n",
    "            normal_df_concat = normal_df_concat.loc[:, normal_df_concat.columns.notnull()]\n",
    "        normal_dfs.append(normal_df_concat)\n",
    "\n",
    "        for n_car in range(n_abnormal):\n",
    "            abnormal_dict = []\n",
    "            for n_snap in range(len(selected_durations[\"abnormal\"][n_car])):\n",
    "                bin_tmp = pd.cut(samples[\"abnormal\"][n_car][n_snap], bins, labels=False)\n",
    "                abnormal_dict.append(Counter(bin_tmp))\n",
    "            abnormal_df = pd.DataFrame(abnormal_dict)\n",
    "            abnormal_df.rename(columns=bin_names, inplace=True)\n",
    "            if n == n_variables - 1:\n",
    "                abnormal_df[\"no\"] = n_normal + n_car\n",
    "                abnormal_df[\"time\"] = selected_durations[\"abnormal\"][n_car].values()\n",
    "                abnormal_df[\"snap\"] = range(len(selected_durations[\"abnormal\"][n_car]))\n",
    "                abnormal_df[\"date\"] = abnormal_df[\"time\"].cumsum()\n",
    "            if n_car == 0:\n",
    "                abnormal_df_concat = abnormal_df\n",
    "            else:\n",
    "                abnormal_df_concat = pd.concat((abnormal_df_concat, abnormal_df))\n",
    "            abnormal_df_concat = abnormal_df_concat.loc[:, abnormal_df_concat.columns.notnull()]\n",
    "        abnormal_dfs.append(abnormal_df_concat)\n",
    "\n",
    "    # clear for memory stability\n",
    "    del samples\n",
    "    normal_com = pd.concat(normal_dfs, axis=1, join='inner')\n",
    "    abnormal_com = pd.concat(abnormal_dfs, axis=1, join='inner')\n",
    "    normal_com['status'] = 0\n",
    "    abnormal_com['status'] = 1\n",
    "    \n",
    "    # index modification\n",
    "    data = pd.concat([normal_com, abnormal_com], axis=0).reset_index(drop=True)\n",
    "    data_simplified = data[columns]\n",
    "    data_simplified.columns = pd.MultiIndex.from_tuples(multicolumns)\n",
    "    data_simplified[['no','time','snap','date','status']] = data[['no','time','snap','date','status']]\n",
    "    \n",
    "    return data_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 1: weight mean 1% increase with 30% prob. std fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn1 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0.5,\n",
    "    \"prob_std\": 0,\n",
    "    \"weight_mean\": 1.01,\n",
    "    \"weight_std\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9', 10: 'A10', 11: 'A11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9', 10: 'C10', 11: 'C11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9', 10: 'D10', 11: 'D11', 12: 'D12', 13: 'D13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9', 10: 'E10', 11: 'E11', 12: 'E12', 13: 'E13'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data1 = createSyntheticDataset(**param_syn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 2: weight mean 5% increase with 30% prob. std fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn2 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0.5,\n",
    "    \"prob_std\": 0,\n",
    "    \"weight_mean\": 1.05,\n",
    "    \"weight_std\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9', 10: 'A10', 11: 'A11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9', 10: 'B10', 11: 'B11', 12: 'B12', 13: 'B13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9', 10: 'D10', 11: 'D11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9', 10: 'E10', 11: 'E11', 12: 'E12', 13: 'E13'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data2 = createSyntheticDataset(**param_syn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 3: weight mean 10% increase with 30% prob. std fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn3 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0.5,\n",
    "    \"prob_std\": 0,\n",
    "    \"weight_mean\": 1.1,\n",
    "    \"weight_std\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9', 10: 'A10', 11: 'A11', 12: 'A12', 13: 'A13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9', 10: 'B10', 11: 'B11', 12: 'B12', 13: 'B13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9', 10: 'C10', 11: 'C11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9', 10: 'D10', 11: 'D11', 12: 'D12', 13: 'D13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9', 10: 'E10', 11: 'E11', 12: 'E12', 13: 'E13'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data3 = createSyntheticDataset(**param_syn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 4: weight std 1% increase with 50% prob. mean fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn4 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0,\n",
    "    \"prob_std\": 0.5,\n",
    "    \"weight_mean\": 1.0,\n",
    "    \"weight_std\": 1.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9', 10: 'A10', 11: 'A11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9', 10: 'C10', 11: 'C11', 12: 'C12', 13: 'C13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data4 = createSyntheticDataset(**param_syn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 5: weight std 5% increase with 50% prob. mean fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn5 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0,\n",
    "    \"prob_std\": 0.5,\n",
    "    \"weight_mean\": 1.0,\n",
    "    \"weight_std\": 1.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9', 10: 'C10', 11: 'C11', 12: 'C12', 13: 'C13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9', 10: 'D10', 11: 'D11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9', 10: 'E10', 11: 'E11', 12: 'E12', 13: 'E13'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data5 = createSyntheticDataset(**param_syn5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data 6: weight std 10% increase with 50% prob. mean fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_syn6 = {\n",
    "    \"n_variables\": 5, # number of histogram variables\n",
    "    \"bin_nos\": [10, 12, 14], #will be randomly chosen\n",
    "    \"durations\": [360, 720, 1080], #will be randomly chosen\n",
    "    \"n_snapshots\": [10, 15, 20],\n",
    "    \"n_normal\": 1000,\n",
    "    \"n_abnormal\": 500,\n",
    "    \"prob_mean\": 0,\n",
    "    \"prob_std\": 0.5,\n",
    "    \"weight_mean\": 1.0,\n",
    "    \"weight_std\": 1.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot & duration creation complete\n",
      "For variable 0 , the bins are {0: 'A0', 1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5', 6: 'A6', 7: 'A7', 8: 'A8', 9: 'A9'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 1 , the bins are {0: 'B0', 1: 'B1', 2: 'B2', 3: 'B3', 4: 'B4', 5: 'B5', 6: 'B6', 7: 'B7', 8: 'B8', 9: 'B9', 10: 'B10', 11: 'B11', 12: 'B12', 13: 'B13'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 2 , the bins are {0: 'C0', 1: 'C1', 2: 'C2', 3: 'C3', 4: 'C4', 5: 'C5', 6: 'C6', 7: 'C7', 8: 'C8', 9: 'C9', 10: 'C10', 11: 'C11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 3 , the bins are {0: 'D0', 1: 'D1', 2: 'D2', 3: 'D3', 4: 'D4', 5: 'D5', 6: 'D6', 7: 'D7', 8: 'D8', 9: 'D9', 10: 'D10', 11: 'D11'}\n",
      "=================================\n",
      "Starting sampling...\n",
      "For variable 4 , the bins are {0: 'E0', 1: 'E1', 2: 'E2', 3: 'E3', 4: 'E4', 5: 'E5', 6: 'E6', 7: 'E7', 8: 'E8', 9: 'E9', 10: 'E10', 11: 'E11'}\n",
      "=================================\n",
      "Starting sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "data6 = createSyntheticDataset(**param_syn6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAnalysis(z, constraints_1 = [0.05, 0, 2000],\n",
    "               constraints_2 = [0, 0, 2000], timeout = 20000000):\n",
    "    \n",
    "    #abnormal set\n",
    "    database1 = Database(z.repair_intervals_removed)\n",
    "    constraints1 = makeConstraints(constraints_1, z.repair_intervals_removed)\n",
    "    algorithm1 = ZMiner(database1, constraints1, forgettable=True)\n",
    "    algorithm1.constraints[\"timeoutseconds\"] = timeout\n",
    "    count, freq, timedelta, timeout1, FL_repair = algorithm1.ZMiner()\n",
    "    \n",
    "    #normal set\n",
    "    database2 = Database(z.normal_intervals_removed)\n",
    "    constraints2 = makeConstraints(constraints_2, z.normal_intervals_removed)\n",
    "    algorithm2 = ZMinerD(database2, constraints2, FL_repair, forgettable=True)\n",
    "    algorithm2.constraints[\"timeoutseconds\"] = timeout\n",
    "    count2, freq2, tdelta2, timeout2, FL_normal = algorithm2.ZMiner()\n",
    "    \n",
    "    return FL_normal, FL_repair, algorithm1.constraints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAnalysis(set1, set2, size_set1, size_set2, filename, constraints):\n",
    "    \n",
    "    #Constraints are just used to generate a file name\n",
    "    for j in set1[2]:\n",
    "        for k in set1[2][j]:\n",
    "            set1[2][j][k] = len(set1[2][j][k])\n",
    "\n",
    "    for j in set2[2]:\n",
    "        for k in set2[2][j]:\n",
    "            set2[2][j][k] = len(set2[2][j][k])\n",
    "    \n",
    "    exportDisprop(filename, set1, set2, size_set1, size_set2, constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_excel(\"Synthetic_data_new1.xlsx\")\n",
    "data2.to_excel(\"Synthetic_data_new2.xlsx\")\n",
    "data3.to_excel(\"Synthetic_data_new3.xlsx\")\n",
    "data4.to_excel(\"Synthetic_data_new4.xlsx\")\n",
    "data5.to_excel(\"Synthetic_data_new5.xlsx\")\n",
    "data6.to_excel(\"Synthetic_data_new6.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename, columns = ['no', 'time', 'snap', 'date', 'status']):\n",
    "    data_loadtest = pd.read_excel(filename, header=[0, 1], index_col=0)\n",
    "    data_copied = data_loadtest[columns]\n",
    "    data_copied.columns = columns\n",
    "    a = data_loadtest.drop(columns, axis=1)\n",
    "    a[columns] = data_copied\n",
    "    a = a.fillna(0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsv/zele5930/.local/lib/python3.6/site-packages/pandas/core/generic.py:3887: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "data1 = loadData(\"Synthetic_data_new1.xlsx\")\n",
    "data2 = loadData(\"Synthetic_data_new2.xlsx\")\n",
    "data3 = loadData(\"Synthetic_data_new3.xlsx\")\n",
    "data4 = loadData(\"Synthetic_data_new4.xlsx\")\n",
    "data5 = loadData(\"Synthetic_data_new5.xlsx\")\n",
    "data6 = loadData(\"Synthetic_data_new6.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.2003814519848675\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 217.06658401200548\n",
      "interval separation started\n",
      "interval separation is done: 0.01011984795331955\n"
     ]
    }
   ],
   "source": [
    "z1 = ZHist(data1, ['no','time','snap','date','status'])\n",
    "z1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.2087526749819517\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 217.27154814917594\n",
      "interval separation started\n",
      "interval separation is done: 0.010706569999456406\n"
     ]
    }
   ],
   "source": [
    "z2 = ZHist(data2, ['no','time','snap','date','status'])\n",
    "z2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.2045948880258948\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 211.9140679340344\n",
      "interval separation started\n",
      "interval separation is done: 0.011008181143552065\n"
     ]
    }
   ],
   "source": [
    "z3 = ZHist(data3, ['no','time','snap','date','status'])\n",
    "z3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.8409740149509162\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 264.2915248409845\n",
      "interval separation started\n",
      "interval separation is done: 0.009996433975175023\n"
     ]
    }
   ],
   "source": [
    "z4 = ZHist(data4, ['no','time','snap','date','status'])\n",
    "z4.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.8548737389501184\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 260.1238736829255\n",
      "interval separation started\n",
      "interval separation is done: 0.009955997113138437\n"
     ]
    }
   ],
   "source": [
    "z5 = ZHist(data5, ['no','time','snap','date','status'])\n",
    "z5.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.8592853820882738\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 260.16304202890024\n",
      "interval separation started\n",
      "interval separation is done: 0.011884032050147653\n"
     ]
    }
   ],
   "source": [
    "z6 = ZHist(data6, ['no','time','snap','date','status'])\n",
    "z6.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getWeightedAverage: 1.2041378929279745\n",
      "interval creation started\n",
      "current: A\n",
      "current: B\n",
      "current: C\n",
      "current: D\n",
      "current: E\n",
      "interval creation is done: 215.02100859908387\n",
      "interval separation started\n",
      "interval separation is done: 0.010951247066259384\n"
     ]
    }
   ],
   "source": [
    "z7 = ZHist(data7, ['no','time','snap','date','status'])\n",
    "z7.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 2888251\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8134\n",
      "5. TOTAL TIME CONSUMED: 42.204905441001756\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 1970949\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8134\n",
      "5. TOTAL TIME CONSUMED: 30.26906665199931\n"
     ]
    }
   ],
   "source": [
    "FL_normal_1, FL_repair_1, c1 = runAnalysis(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 2978262\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8132\n",
      "5. TOTAL TIME CONSUMED: 42.94829656699949\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 2037866\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8132\n",
      "5. TOTAL TIME CONSUMED: 30.838913900002808\n"
     ]
    }
   ],
   "source": [
    "FL_normal_2, FL_repair_2, c2 = runAnalysis(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 2912834\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8075\n",
      "5. TOTAL TIME CONSUMED: 41.307417665000685\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 1970069\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 8075\n",
      "5. TOTAL TIME CONSUMED: 30.296202350000385\n"
     ]
    }
   ],
   "source": [
    "FL_normal_3, FL_repair_3, c3 = runAnalysis(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 8612111\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 21888\n",
      "5. TOTAL TIME CONSUMED: 109.52835602899722\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 1629374\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 15704\n",
      "5. TOTAL TIME CONSUMED: 27.213986329999898\n"
     ]
    }
   ],
   "source": [
    "FL_normal_4, FL_repair_4, c4 = runAnalysis(z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 8941354\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 24188\n",
      "5. TOTAL TIME CONSUMED: 117.38165531999766\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 1604464\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 17772\n",
      "5. TOTAL TIME CONSUMED: 30.078794281002047\n"
     ]
    }
   ],
   "source": [
    "FL_normal_5, FL_repair_5, c5 = runAnalysis(z5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 25.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "1-5. LEVEL: inf\n",
      "2. NUMBER OF E-SEQUENCES: 500\n",
      "3. TOTAL COMPARISON COUNTS: 8849820\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 23023\n",
      "5. TOTAL TIME CONSUMED: 113.03075962199728\n",
      "########## Z-MINER ##########\n",
      "1-1. MINIMUM SUPPORT: 0.0\n",
      "1-2. EPSILON CONSTRAINT: 0.0\n",
      "1-3. GAP CONSTRAINT: 2000.0\n",
      "1-4. TIMEOUT: 20000000\n",
      "2. NUMBER OF E-SEQUENCES: 1000\n",
      "3. TOTAL COMPARISON COUNTS: 2183548\n",
      "4. TOTAL FREQUENT ARRANGEMENTS: 20199\n",
      "5. TOTAL TIME CONSUMED: 41.325332876000175\n"
     ]
    }
   ],
   "source": [
    "FL_normal_6, FL_repair_6, c6 = runAnalysis(z6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAnalysis(set1, set2, size_set1, size_set2, filename, constraints, lenFix=True):\n",
    "    \n",
    "    if lenFix==True:\n",
    "        #Constraints are just used to generate a file name\n",
    "        for j in set1[2]:\n",
    "            for k in set1[2][j]:\n",
    "                set1[2][j][k] = len(set1[2][j][k])\n",
    "\n",
    "        for j in set2[2]:\n",
    "            for k in set2[2][j]:\n",
    "                set2[2][j][k] = len(set2[2][j][k])\n",
    "\n",
    "    exportDisprop(filename, set1, set2, size_set1, size_set2, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenFix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_1, FL_normal_1, 500, 1000, \"Synthetic_1_new\", c1, lenFix=lenFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_2, FL_normal_2, 500, 1000, \"Synthetic_2_new\", c2, lenFix=lenFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_3, FL_normal_3, 500, 1000, \"Synthetic_3_new\", c3, lenFix=lenFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_4, FL_normal_4, 500, 1000, \"Synthetic_4_new\", c4, lenFix=lenFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_5, FL_normal_5, 500, 1000, \"Synthetic_5_new\", c5, lenFix=lenFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAnalysis(FL_repair_6, FL_normal_6, 500, 1000, \"Synthetic_6_new\", c6, lenFix=lenFix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
